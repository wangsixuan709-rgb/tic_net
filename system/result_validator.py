import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple, Any
import logging
from .annotation_handler import AnnotationHandler
import os
import matplotlib.patches as patches

class ResultValidator:
    """æ£€æµ‹ç»“æœéªŒè¯å™¨ï¼Œç”¨äºå¯¹æ¯”æ£€æµ‹ç»“æœä¸çœŸå®æ ‡æ³¨"""
    
    def __init__(self, annotation_handler: AnnotationHandler = None):
        self.logger = self._setup_logger()
        self.annotation_handler = annotation_handler or AnnotationHandler()
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('ResultValidator')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def calculate_iou_3d(self, box1: List[float], box2: List[float]) -> float:
        """è®¡ç®—3D IoU (Intersection over Union)"""
        try:
            # boxæ ¼å¼: [x1, y1, z1, x2, y2, z2]
            x1_inter = max(box1[0], box2[0])
            y1_inter = max(box1[1], box2[1])
            z1_inter = max(box1[2], box2[2])
            x2_inter = min(box1[3], box2[3])
            y2_inter = min(box1[4], box2[4])
            z2_inter = min(box1[5], box2[5])
            
            # è®¡ç®—äº¤é›†ä½“ç§¯
            if x2_inter > x1_inter and y2_inter > y1_inter and z2_inter > z1_inter:
                intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter) * (z2_inter - z1_inter)
            else:
                intersection = 0.0
            
            # è®¡ç®—å„è‡ªä½“ç§¯
            volume1 = (box1[3] - box1[0]) * (box1[4] - box1[1]) * (box1[5] - box1[2])
            volume2 = (box2[3] - box2[0]) * (box2[4] - box2[1]) * (box2[5] - box2[2])
            
            # è®¡ç®—å¹¶é›†ä½“ç§¯
            union = volume1 + volume2 - intersection
            
            return intersection / union if union > 0 else 0.0
            
        except Exception as e:
            self.logger.warning(f"IoUè®¡ç®—å¤±è´¥: {str(e)}")
            return 0.0
    
    def calculate_distance_3d(self, center1: List[float], center2: List[float]) -> float:
        """è®¡ç®—3Dæ¬§æ°è·ç¦»"""
        try:
            dx = center1[0] - center2[0]
            dy = center1[1] - center2[1]
            dz = center1[2] - center2[2]
            return np.sqrt(dx*dx + dy*dy + dz*dz)
        except:
            return float('inf')
    
    def validate_detection_results(self, 
                                 image_path: str,
                                 detections: List[Dict],
                                 meta_info: Dict,
                                 iou_threshold: float = 0.3,
                                 distance_threshold: float = 10.0) -> Dict[str, Any]:
        """éªŒè¯æ£€æµ‹ç»“æœä¸çœŸå®æ ‡æ³¨çš„å¯¹æ¯”"""
        try:
            self.logger.info(f"æ­£åœ¨éªŒè¯æ£€æµ‹ç»“æœ: {len(detections)} ä¸ªæ£€æµ‹")
            
            # è·å–çœŸå®æ ‡æ³¨
            truth_boxes, truth_labels = self.annotation_handler.get_truth_data_for_image(
                image_path,
                meta_info['spacing'],
                meta_info['origin'], 
                meta_info['original_shape']
            )
            
            self.logger.info(f"çœŸå®æ ‡æ³¨æ•°é‡: {len(truth_boxes)}")
            
            if not truth_boxes:
                return {
                    'has_ground_truth': False,
                    'message': 'è¯¥å›¾åƒæ²¡æœ‰çœŸå®æ ‡æ³¨ï¼Œæ— æ³•è¿›è¡ŒéªŒè¯',
                    'detection_count': len(detections),
                    'ground_truth_count': 0
                }
            
            # è½¬æ¢çœŸå®æ ‡æ³¨ä¸ºæ£€æµ‹æ ¼å¼
            ground_truth = []
            for i, (box, label) in enumerate(zip(truth_boxes, truth_labels)):
                center_x = (box[0] + box[3]) / 2
                center_y = (box[1] + box[4]) / 2  
                center_z = (box[2] + box[5]) / 2
                
                gt = {
                    'bbox': box,
                    'center': [center_x, center_y, center_z],
                    'label': label,
                    'id': i
                }
                ground_truth.append(gt)
            
            # åŒ¹é…æ£€æµ‹ç»“æœä¸çœŸå®æ ‡æ³¨
            matches = []
            unmatched_detections = list(range(len(detections)))
            unmatched_ground_truth = list(range(len(ground_truth)))
            
            # ä¸ºæ¯ä¸ªæ£€æµ‹æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„çœŸå®æ ‡æ³¨
            for det_idx, detection in enumerate(detections):
                best_match = None
                best_score = 0.0
                best_gt_idx = -1
                
                det_center = detection.get('center', [
                    (detection['bbox'][0] + detection['bbox'][3]) / 2,
                    (detection['bbox'][1] + detection['bbox'][4]) / 2,
                    (detection['bbox'][2] + detection['bbox'][5]) / 2
                ])
                
                for gt_idx in unmatched_ground_truth:
                    gt = ground_truth[gt_idx]
                    
                    # è®¡ç®—IoU
                    iou = self.calculate_iou_3d(detection['bbox'], gt['bbox'])
                    
                    # è®¡ç®—è·ç¦»
                    distance = self.calculate_distance_3d(det_center, gt['center'])
                    
                    # ç»¼åˆè¯„åˆ† (IoUæƒé‡æ›´é«˜)
                    score = iou * 0.7 + (1.0 / (1.0 + distance/10.0)) * 0.3
                    
                    if (iou >= iou_threshold or distance <= distance_threshold) and score > best_score:
                        best_score = score
                        best_match = {
                            'detection_idx': det_idx,
                            'ground_truth_idx': gt_idx,
                            'iou': iou,
                            'distance': distance,
                            'score': score,
                            'confidence': detection['confidence']
                        }
                        best_gt_idx = gt_idx
                
                if best_match:
                    matches.append(best_match)
                    if det_idx in unmatched_detections:
                        unmatched_detections.remove(det_idx)
                    if best_gt_idx in unmatched_ground_truth:
                        unmatched_ground_truth.remove(best_gt_idx)
            
            # è®¡ç®—æŒ‡æ ‡
            true_positives = len(matches)
            false_positives = len(unmatched_detections)
            false_negatives = len(unmatched_ground_truth)
            
            precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
            recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            # è®¡ç®—å¹³å‡IoUå’Œè·ç¦»
            avg_iou = np.mean([m['iou'] for m in matches]) if matches else 0
            avg_distance = np.mean([m['distance'] for m in matches]) if matches else 0
            avg_confidence = np.mean([m['confidence'] for m in matches]) if matches else 0
            
            validation_result = {
                'has_ground_truth': True,
                'detection_count': len(detections),
                'ground_truth_count': len(ground_truth),
                'true_positives': true_positives,
                'false_positives': false_positives,
                'false_negatives': false_negatives,
                'precision': precision,
                'recall': recall,
                'f1_score': f1_score,
                'average_iou': avg_iou,
                'average_distance': avg_distance,
                'average_confidence': avg_confidence,
                'matches': matches,
                'unmatched_detections': [detections[i] for i in unmatched_detections],
                'unmatched_ground_truth': [ground_truth[i] for i in unmatched_ground_truth],
                'validation_summary': self._generate_validation_summary(
                    true_positives, false_positives, false_negatives, 
                    precision, recall, f1_score, avg_iou
                )
            }
            
            self.logger.info(f"éªŒè¯å®Œæˆ: TP={true_positives}, FP={false_positives}, FN={false_negatives}")
            self.logger.info(f"ç²¾åº¦={precision:.3f}, å¬å›ç‡={recall:.3f}, F1={f1_score:.3f}")
            
            return validation_result
            
        except Exception as e:
            self.logger.error(f"éªŒè¯å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                'has_ground_truth': False,
                'error': str(e),
                'message': 'éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯'
            }
    
    def _generate_validation_summary(self, tp: int, fp: int, fn: int, 
                                   precision: float, recall: float, 
                                   f1_score: float, avg_iou: float) -> str:
        """ç”ŸæˆéªŒè¯æ‘˜è¦æ–‡å­—"""
        if tp == 0 and fp == 0 and fn == 0:
            return "æ²¡æœ‰æ£€æµ‹ç»“æœå’ŒçœŸå®æ ‡æ³¨"
        
        summary_parts = []
        
        # æ€»ä½“è¯„ä»·
        if f1_score >= 0.8:
            summary_parts.append("ğŸŸ¢ æ£€æµ‹æ•ˆæœä¼˜ç§€")
        elif f1_score >= 0.6:
            summary_parts.append("ğŸŸ¡ æ£€æµ‹æ•ˆæœè‰¯å¥½") 
        elif f1_score >= 0.4:
            summary_parts.append("ğŸŸ  æ£€æµ‹æ•ˆæœä¸€èˆ¬")
        else:
            summary_parts.append("ğŸ”´ æ£€æµ‹æ•ˆæœè¾ƒå·®")
        
        # å…·ä½“åˆ†æ
        if precision >= 0.8:
            summary_parts.append("ç²¾åº¦é«˜ï¼Œè¯¯æŠ¥è¾ƒå°‘")
        elif precision >= 0.5:
            summary_parts.append("ç²¾åº¦ä¸­ç­‰ï¼Œæœ‰ä¸€å®šè¯¯æŠ¥")
        else:
            summary_parts.append("ç²¾åº¦è¾ƒä½ï¼Œè¯¯æŠ¥è¾ƒå¤š")
        
        if recall >= 0.8:
            summary_parts.append("å¬å›ç‡é«˜ï¼Œæ¼æ£€è¾ƒå°‘")
        elif recall >= 0.5:
            summary_parts.append("å¬å›ç‡ä¸­ç­‰ï¼Œæœ‰ä¸€å®šæ¼æ£€")
        else:
            summary_parts.append("å¬å›ç‡è¾ƒä½ï¼Œæ¼æ£€è¾ƒå¤š")
        
        if avg_iou >= 0.5:
            summary_parts.append("å®šä½ç²¾ç¡®")
        elif avg_iou >= 0.3:
            summary_parts.append("å®šä½åŸºæœ¬å‡†ç¡®")
        else:
            summary_parts.append("å®šä½ç²¾åº¦æœ‰å¾…æé«˜")
        
        return "ï¼›".join(summary_parts)
    
    def create_comparison_visualization(self, 
                                      image_data: np.ndarray,
                                      detections: List[Dict],
                                      validation_result: Dict,
                                      task_id: str,
                                      save_dir: str) -> str:
        """åˆ›å»ºæ£€æµ‹ç»“æœä¸çœŸå®æ ‡æ³¨çš„å¯¹æ¯”å¯è§†åŒ–"""
        try:
            if not validation_result.get('has_ground_truth', False):
                return ""
            
            fig = plt.figure(figsize=(20, 12))
            
            # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§åˆ‡ç‰‡
            depth = image_data.shape[0]
            slice_indices = [depth // 4, depth // 2, 3 * depth // 4]
            
            # å¸ƒå±€ï¼šä¸Šè¡Œæ˜¾ç¤ºæ£€æµ‹ç»“æœï¼Œä¸‹è¡Œæ˜¾ç¤ºå¯¹æ¯”åˆ†æ
            gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.2)
            
            # ä¸Šè¡Œï¼šæ˜¾ç¤ºæ£€æµ‹ç»“æœå åŠ å›¾
            for i, slice_idx in enumerate(slice_indices):
                ax = fig.add_subplot(gs[0, i])
                ax.imshow(image_data[slice_idx], cmap='gray')
                ax.set_title(f'åˆ‡ç‰‡ {slice_idx} - æ£€æµ‹ç»“æœå¯¹æ¯”', fontsize=12)
                ax.axis('off')
                
                # ç»˜åˆ¶çœŸå®æ ‡æ³¨ï¼ˆç»¿è‰²ï¼‰
                for gt in validation_result.get('unmatched_ground_truth', []):
                    bbox = gt['bbox']
                    if bbox[2] <= slice_idx <= bbox[5]:
                        rect = patches.Rectangle(
                            (bbox[0], bbox[1]), bbox[3] - bbox[0], bbox[4] - bbox[1],
                            linewidth=2, edgecolor='green', facecolor='none', 
                            linestyle='--', label='çœŸå®æ ‡æ³¨(æœªåŒ¹é…)'
                        )
                        ax.add_patch(rect)
                
                # ç»˜åˆ¶åŒ¹é…çš„æ£€æµ‹ï¼ˆè“è‰²ï¼‰å’ŒçœŸå®æ ‡æ³¨ï¼ˆç»¿è‰²ï¼‰
                for match in validation_result.get('matches', []):
                    det_idx = match['detection_idx']
                    detection = detections[det_idx]
                    bbox = detection['bbox']
                    
                    if bbox[2] <= slice_idx <= bbox[5]:
                        # æ£€æµ‹æ¡†ï¼ˆè“è‰²ï¼‰
                        rect = patches.Rectangle(
                            (bbox[0], bbox[1]), bbox[3] - bbox[0], bbox[4] - bbox[1],
                            linewidth=2, edgecolor='blue', facecolor='none'
                        )
                        ax.add_patch(rect)
                        
                        # æ·»åŠ ç½®ä¿¡åº¦å’ŒIoU
                        ax.text(bbox[0], bbox[1] - 5, 
                               f'æ£€æµ‹: {detection["confidence"]:.2f}\nIoU: {match["iou"]:.2f}',
                               color='blue', fontsize=8, fontweight='bold')
                
                # ç»˜åˆ¶æœªåŒ¹é…çš„æ£€æµ‹ï¼ˆçº¢è‰²ï¼‰
                for i in range(len(detections)):
                    if i in [m['detection_idx'] for m in validation_result.get('matches', [])]:
                        continue
                    
                    detection = detections[i]
                    bbox = detection['bbox']
                    if bbox[2] <= slice_idx <= bbox[5]:
                        rect = patches.Rectangle(
                            (bbox[0], bbox[1]), bbox[3] - bbox[0], bbox[4] - bbox[1],
                            linewidth=2, edgecolor='red', facecolor='none',
                            linestyle=':'
                        )
                        ax.add_patch(rect)
                        ax.text(bbox[0], bbox[1] - 5, f'è¯¯æŠ¥: {detection["confidence"]:.2f}',
                               color='red', fontsize=8)
            
            # å³ä¾§ï¼šéªŒè¯æŒ‡æ ‡
            ax_metrics = fig.add_subplot(gs[0, 3])
            ax_metrics.axis('off')
            metrics_text = f"""éªŒè¯ç»“æœ:
            
çœŸå®æ ‡æ³¨: {validation_result['ground_truth_count']}
æ£€æµ‹ç»“æœ: {validation_result['detection_count']}

æ­£ç¡®æ£€æµ‹: {validation_result['true_positives']}
è¯¯æŠ¥: {validation_result['false_positives']}  
æ¼æ£€: {validation_result['false_negatives']}

ç²¾åº¦: {validation_result['precision']:.3f}
å¬å›ç‡: {validation_result['recall']:.3f}
F1åˆ†æ•°: {validation_result['f1_score']:.3f}

å¹³å‡IoU: {validation_result['average_iou']:.3f}
å¹³å‡è·ç¦»: {validation_result['average_distance']:.1f}

{validation_result['validation_summary']}"""
            
            ax_metrics.text(0.05, 0.95, metrics_text, transform=ax_metrics.transAxes,
                           fontsize=11, verticalalignment='top', fontfamily='monospace')
            
            # ä¸‹è¡Œï¼šè¯¦ç»†åˆ†æå›¾è¡¨
            # ç½®ä¿¡åº¦åˆ†å¸ƒ
            ax_conf = fig.add_subplot(gs[1, 0])
            if detections:
                confidences = [d['confidence'] for d in detections]
                ax_conf.hist(confidences, bins=10, alpha=0.7, color='skyblue', edgecolor='black')
                ax_conf.set_title('æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
                ax_conf.set_xlabel('ç½®ä¿¡åº¦')
                ax_conf.set_ylabel('æ•°é‡')
            
            # IoUåˆ†å¸ƒï¼ˆä»…åŒ¹é…çš„ï¼‰
            ax_iou = fig.add_subplot(gs[1, 1])
            if validation_result.get('matches'):
                ious = [m['iou'] for m in validation_result['matches']]
                ax_iou.hist(ious, bins=10, alpha=0.7, color='lightgreen', edgecolor='black')
                ax_iou.set_title('IoUåˆ†å¸ƒï¼ˆåŒ¹é…æ£€æµ‹ï¼‰')
                ax_iou.set_xlabel('IoU')
                ax_iou.set_ylabel('æ•°é‡')
            
            # æ··æ·†çŸ©é˜µé£æ ¼çš„æ€»ç»“
            ax_summary = fig.add_subplot(gs[1, 2:])
            categories = ['æ­£ç¡®æ£€æµ‹', 'è¯¯æŠ¥', 'æ¼æ£€']
            values = [validation_result['true_positives'], 
                     validation_result['false_positives'],
                     validation_result['false_negatives']]
            colors = ['green', 'red', 'orange']
            
            bars = ax_summary.bar(categories, values, color=colors, alpha=0.7)
            ax_summary.set_title('æ£€æµ‹ç»“æœæ±‡æ€»')
            ax_summary.set_ylabel('æ•°é‡')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax_summary.text(bar.get_x() + bar.get_width()/2., height,
                               f'{value}', ha='center', va='bottom', fontweight='bold')
            
            plt.suptitle(f'æ£€æµ‹ç»“æœéªŒè¯æŠ¥å‘Š - ä»»åŠ¡ {task_id[:8]}', fontsize=16)
            
            # ä¿å­˜å›¾åƒ
            filename = f"{task_id}_validation.png"
            save_path = os.path.join(save_dir, filename)
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            return filename
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return "" 