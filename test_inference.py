#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ¨¡å‹æ¨ç†åŠŸèƒ½
"""

import sys
import os
import torch
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

from system.config import SystemConfig
from system.model_inference import ModelInference

def test_model_inference():
    """æµ‹è¯•æ¨¡å‹æ¨ç†åŠŸèƒ½"""
    print("=" * 60)
    print("TiCNet æ¨¡å‹æ¨ç†æµ‹è¯•")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–é…ç½®å’Œæ¨ç†å¼•æ“
        config = SystemConfig()
        print(f"è®¾å¤‡: {config.get_device()}")
        
        # åˆå§‹åŒ–æ¨¡å‹æ¨ç†
        print("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹æ¨ç†å¼•æ“...")
        inference = ModelInference(config)
        print("âœ… æ¨¡å‹æ¨ç†å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        print("\næ­£åœ¨åˆ›å»ºæµ‹è¯•æ•°æ®...")
        
        # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„å›¾åƒæ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨LUNA16æ ¼å¼çš„seriesuidï¼‰
        test_seriesuid = "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860"
        test_image_path = f"test_data/{test_seriesuid}.nrrd"
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„å›¾åƒæ•°æ®
        test_image = np.random.randint(0, 255, (64, 128, 128), dtype=np.int16)
        
        # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
        test_dir = Path("test_data")
        test_dir.mkdir(exist_ok=True)
        
        print(f"æµ‹è¯•å›¾åƒè·¯å¾„: {test_image_path}")
        print(f"æµ‹è¯•å›¾åƒå½¢çŠ¶: {test_image.shape}")
        
        # æ¨¡æ‹Ÿmeta_info
        meta_info = {
            'spacing': (1.0, 1.0, 1.0),
            'origin': (0.0, 0.0, 0.0),
            'original_shape': test_image.shape,
            'dtype': str(test_image.dtype)
        }
        
        # æµ‹è¯•æ³¨è§£å¤„ç†å™¨
        print("\næµ‹è¯•æ³¨è§£å¤„ç†...")
        seriesuid = inference.annotation_handler.extract_seriesuid_from_path(test_image_path)
        print(f"æå–çš„seriesuid: {seriesuid}")
        
        if seriesuid:
            annotations = inference.annotation_handler.get_annotations_for_seriesuid(seriesuid)
            print(f"æ‰¾åˆ°çš„æ³¨è§£æ•°é‡: {len(annotations)}")
            
            truth_boxes, truth_labels = inference.annotation_handler.get_truth_data_for_image(
                test_image_path, 
                meta_info['spacing'], 
                meta_info['origin'], 
                meta_info['original_shape']
            )
            print(f"Truth boxesæ•°é‡: {len(truth_boxes)}")
            print(f"Truth labelsæ•°é‡: {len(truth_labels)}")
        
        print("\nâœ… æ³¨è§£å¤„ç†æµ‹è¯•å®Œæˆ")
        
        # æµ‹è¯•æ¨¡å‹æ¨ç†ï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰
        print("\næµ‹è¯•æ¨¡å‹æ¨ç†...")
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„å›¾åƒå¼ é‡
        image_tensor = torch.from_numpy(test_image.astype(np.float32))
        image_tensor = image_tensor.unsqueeze(0).unsqueeze(0)  # æ·»åŠ batchå’Œchannelç»´åº¦
        image_tensor = image_tensor.to(inference.device)
        
        print(f"è¾“å…¥å¼ é‡å½¢çŠ¶: {image_tensor.shape}")
        
        # æµ‹è¯•æ¨¡å‹forwardè°ƒç”¨
        try:
            inference.model.set_mode('eval')
            
            # åˆ›å»ºç©ºçš„truthæ•°æ®ç”¨äºæµ‹è¯•
            truth_boxes_list = [torch.zeros((0, 6), dtype=torch.float32, device=inference.device)]
            truth_labels_list = [torch.zeros((0,), dtype=torch.long, device=inference.device)]
            
            print("æ­£åœ¨è°ƒç”¨æ¨¡å‹...")
            with torch.no_grad():
                inference.model.forward(image_tensor, truth_boxes_list, truth_labels_list)
            
            print("âœ… æ¨¡å‹forwardè°ƒç”¨æˆåŠŸ")
            
            # æ£€æŸ¥æ¨¡å‹è¾“å‡º
            if hasattr(inference.model, 'rpn_proposals'):
                print(f"RPN proposals: {inference.model.rpn_proposals.shape if inference.model.rpn_proposals is not None else 'None'}")
            if hasattr(inference.model, 'detections'):
                print(f"Detections: {inference.model.detections.shape if inference.model.detections is not None else 'None'}")
            if hasattr(inference.model, 'ensemble_proposals'):
                print(f"Ensemble proposals: {inference.model.ensemble_proposals.shape if inference.model.ensemble_proposals is not None else 'None'}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹æ¨ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == '__main__':
    success = test_model_inference()
    if success:
        print("\nğŸ‰ æ¨ç†ç³»ç»Ÿæµ‹è¯•æˆåŠŸï¼")
        sys.exit(0)
    else:
        print("\nğŸ’¥ æ¨ç†ç³»ç»Ÿæµ‹è¯•å¤±è´¥ï¼")
        sys.exit(1) 